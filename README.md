We combine multiple predictive techniques: XGBoost, an LSTM-based neural network, and an SVR. On top of that, we use a Gradient Boosting Regressor to figure out how to weight each of these forecasts, along with various technical indicators, fundamental data, options info, and sentiment. That way, we capture a bunch of different market signals, and hopefully, our predictions end up more accurate than if we relied on just one approach.

For features, we grab the usual suspects like RSI, MACD, Bollinger Bands, Stochastics, and VWAP—these capture day-to-day momentum and volatility. We also apply wavelet denoising to the closing price so the model focuses on broader trends, and then we create lag features from that smoother price. We add fundamental metrics like trailing PE, dividend yield, and price-to-book from Yahoo Finance, plus a neat options trick: we look for unusually high-volume calls or puts and record the average strike price (this can be a clue about where big traders expect the stock to move). We also throw in a simple Markov chain probability showing “up” vs. “down” transitions and a sentiment score from news headlines via NLTK’s VADER.

We split the data 80/20, run a grid search on XGBoost (tuning things like n_estimators, learning_rate, max_depth), and let LSTM and SVR each do their part. Our final Gradient Boosting layer then merges them all together. We measure performance by RMSE and do a rolling backtest over the last month or so to see how well it does in semi-real conditions.

Some big takeaways: wavelet denoising helps filter out random noise, fundamentals can be less influential on super-short horizons but still catch big shifts, and sentiment can be messy but occasionally crucial when the market freaks out or cheers something. The options signal can be surprisingly handy—if giant volumes of calls or puts appear at a certain strike, that might be a pivot point. However, it’s all more stable in calmer markets; when volatility spikes, the model can still get caught off-guard.

Down the road, we could add even more data—like macro indicators or social media sentiment—and play with advanced models like Transformers or a dynamic weighting scheme for each sub-model. But for now, our multi-method ensemble seems to handle short-term forecasting pretty well.
